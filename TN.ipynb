{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48b0b62-1d78-4ccd-a26c-60c5ff07856b",
   "metadata": {},
   "source": [
    "# 张量网络并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ed4fb-62cf-416a-b0a1-f5b90b38c53e",
   "metadata": {},
   "source": [
    "## 1.Network类与张量收缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0fbfe7-ce4b-4b1c-ae4f-885dd223ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyang/anaconda3/lib/python3.12/site-packages/cupy/_environment.py:540: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "from cuquantum import Network\n",
    "import numpy as np\n",
    "from cupy.cuda.runtime import getDeviceCount\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "from cuquantum import OptimizerOptions\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5f990-8b48-4aba-b3b3-aa1d29d4d567",
   "metadata": {},
   "source": [
    "## 1.1张量网络定义+路径优化+并行切片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f517d7-8dd5-43df-95d6-8e59c8ee8d02",
   "metadata": {},
   "source": [
    "![jupyter](./f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6751ee6d-f387-4c71-8987-898ecac751f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33554432\n",
      "收缩路径: [(0, 1), (0, 1), (0, 1)]\n",
      "总 FLOPs 数: 1100048564208.0\n",
      "最佳路径: [(0, 1), (0, 1), (0, 1)]\n",
      "切片数: 4096\n",
      "切片模式: (('i', 1),)\n",
      "261.18039631843567\n"
     ]
    }
   ],
   "source": [
    "# 定义张量收缩表达式   \n",
    "expr = 'aij,bjk,klc,lid'\n",
    "# 定义张量形状 \n",
    "n=12\n",
    "shapes = [(2, 2**n,2**n), (2, 2**n, 2**n), (2**n,2**n, 2),(2**n,2**n, 2)]\n",
    "print(2**25)\n",
    "# 准备张量数据   \n",
    "operands = [np.random.rand(*shape) for shape in shapes]\n",
    "\n",
    "\n",
    "# 创建 Network 对象   \n",
    "network = Network(expr, *operands)\n",
    "# 创建 OptimizerOptions 对象  \n",
    "opt_options = OptimizerOptions(samples=16, slicing={'min_slices': 2})\n",
    "# 查看收缩路径  \n",
    "path, info = network.contract_path(optimize=opt_options)\n",
    "print(\"收缩路径:\", path)\n",
    "print(\"总 FLOPs 数:\", info.opt_cost)  # 估算的浮点运算数 \n",
    "print(\"最佳路径:\", info.path)  # 查看最优路径  \n",
    "print(\"切片数:\", info.num_slices)  # 切片数量信息 \n",
    "print(\"切片模式:\", info.slices)  # 切片模式信息 \n",
    "start = time.time()\n",
    "result = network.contract()\n",
    "end = time.time()\n",
    "print (str(end-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec467c79-e94d-4be5-b187-7e5d668f5d47",
   "metadata": {},
   "source": [
    "## 1.2并行运算 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a42752-2a35-4dc1-9f26-31ffbe0cdd41",
   "metadata": {},
   "source": [
    "1) 初始化 MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfd8153-ad0a-4de3-8841-b2939b8d36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "root =0\n",
    "comm = MPI.COMM_WORLD\n",
    "rank, size = comm.Get_rank(), comm.Get_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61871e-3124-4c69-8749-caf82b727f8e",
   "metadata": {},
   "source": [
    "MPI.COMM_WORLD：定义全局通信器，以便所有进程参与同一个通信域。\\\n",
    "rank：当前进程的标识符。\\\n",
    "size：总的进程数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f79f0-63b0-4221-8f95-d7c6e4a673aa",
   "metadata": {},
   "source": [
    "2) 设置张量表达式与形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "859f0927-acad-4fbf-9159-b1400d688e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义张量收缩表达式   \n",
    "expr = 'ij,jkl,lm'\n",
    "# 定义张量形状 \n",
    "shapes = [(2, 2), (2, 2, 2), (2, 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2d472-5316-4169-b06f-7d7c8865a74a",
   "metadata": {},
   "source": [
    "expr: 表示张量网络的收缩表达式。\\\n",
    "shapes: 每个操作数（即张量）的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11983b89-0a38-4fe0-845b-2741e0dea3ea",
   "metadata": {},
   "source": [
    "3) 广播操作数数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1029da3c-6c71-4eb3-9d6b-219032a2b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#广播操作数数据\n",
    "operands = [np.random.rand(*shape)\n",
    "            for shape in shapes] if rank == root else None\n",
    "operands = comm.bcast(operands, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4978e7-113e-4e13-aef2-febc0b83eef1",
   "metadata": {},
   "source": [
    "仅在 rank == root 的进程（通常是主进程）上创建操作数的数据，这些数据随后会通过 MPI 广播给其他进程。\\\n",
    "将主进程的数据广播给所有进程，这样每个进程都有相同的操作数数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955492ae-6055-4253-8e90-7d094361ee16",
   "metadata": {},
   "source": [
    "4) 设备分配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e12653-c2ab-4c9b-9934-fe8f4da339db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分配设备\n",
    "device_id = rank % getDeviceCount()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80647a-feca-44e3-b815-0b06072206a9",
   "metadata": {},
   "source": [
    "使用当前进程 rank 与 GPU 设备总数进行取模，以便每个进程分配一个 GPU 设备。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc881c-435d-496a-8426-d72e522662d0",
   "metadata": {},
   "source": [
    "5) 创建 Network 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac43b19-8418-4f23-a966-0ad697289133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建network对象\n",
    "network = Network(expr, *operands, \n",
    "                  options={'device_id' : device_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e76d56-8054-4a8c-9866-8e0652dfdbc5",
   "metadata": {},
   "source": [
    "使用 Network 类创建张量网络对象，指定收缩表达式和操作数，并分配到指定的 GPU 上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078883b-2838-4508-a03c-67ec93cef4b7",
   "metadata": {},
   "source": [
    "6) 计算收缩路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9f4118-8091-42d8-9427-63a404357d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算收缩路径\n",
    "path, info = network.contract_path(\n",
    "    optimize={'samples': 8, 'slicing': \n",
    "              {'min_slices': size}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2d3d9-e47d-48c6-8599-b68367f6d460",
   "metadata": {},
   "source": [
    "使用 contract_path 方法优化张量收缩路径。\\\n",
    "samples=8 表示超参数优化的采样次数。\\\n",
    "slicing={'min_slices': max(16, size)} 强制启用切片以支持并行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12148085-49ae-44ee-b6c5-0aceac7618f1",
   "metadata": {},
   "source": [
    "7) 选择最佳路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3aead68-1374-46ec-b3ad-0622f0867e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 has the path with the lowest FLOP count 64.0.\n"
     ]
    }
   ],
   "source": [
    "#选择计算开销最小的收缩路径\n",
    "opt_cost, sender = comm.allreduce(\n",
    "    sendobj=(info.opt_cost, rank), op=MPI.MINLOC)\n",
    "if rank == root:\n",
    "    print(f\"Process {sender} has the path with the lowest FLOP count {opt_cost}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3350ec1-cc56-4f1e-9dd9-bfdecfd34136",
   "metadata": {},
   "source": [
    "使用 MPI.MINLOC 操作来选择 FLOP 计算开销最小的路径，并找出拥有此路径的进程 sender。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2205c-694d-4647-bf7d-6d5a3ebd662a",
   "metadata": {},
   "source": [
    "8) 广播最佳路径信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f41ad61-7030-48c1-af7a-53deca304f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 has the path with the lowest FLOP count 64.0.\n"
     ]
    }
   ],
   "source": [
    "#广播与设置收缩路径确保几个进程统一收缩路径\n",
    "opt_cost, sender = comm.allreduce(\n",
    "    sendobj=(info.opt_cost, rank), op=MPI.MINLOC)\n",
    "if rank == root:\n",
    "    print(f\"Process {sender} has the path with the lowest FLOP count {opt_cost}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3986cd-a296-4a2a-8b73-b4f32afbd19c",
   "metadata": {},
   "source": [
    "将最佳路径的信息从 sender 进程广播给其他进程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d53967-3458-4e11-b43e-61e3398bb495",
   "metadata": {},
   "source": [
    "9) 设置路径和切片信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622cf2f8-1f15-488a-9cdc-ac5690eed108",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, info = network.contract_path(\n",
    "    optimize={'path': info.path, 'slicing': info.slices})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0874c481-9cf5-4a48-be86-12c9ccf37389",
   "metadata": {},
   "source": [
    "使用之前计算得到的路径和切片信息重新设置路径，这样每个进程都能获得同样的路径信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c711eb7-b084-4b7d-9e1d-166bdac207cd",
   "metadata": {},
   "source": [
    "10) 切片分配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda0539c-eccf-4478-be37-2f18d24687a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path, info = network.contract_path(\n",
    "    optimize={'path': info.path, 'slicing': info.slices})\n",
    "num_slices = info.num_slices\n",
    "chunk, extra = num_slices // size, num_slices % size\n",
    "slice_begin = rank * chunk + min(rank, extra)\n",
    "slice_end = num_slices if rank == size - 1 else (rank + 1) * chunk + min(rank + 1, extra)\n",
    "slices = range(slice_begin, slice_end)   \n",
    "print(slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e132b638-4b9d-45d5-87ae-c10b71fc62b9",
   "metadata": {},
   "source": [
    "将切片任务分配到每个进程：\\\n",
    "num_slices: 总切片数。\n",
    "slice_begin 和 slice_end: 当前进程负责的切片范围"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35839b09-f7b7-4811-b3db-cca0eba9edb7",
   "metadata": {},
   "source": [
    "11) 执行收缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94b00b22-0094-4c59-a2e0-c521e37ce844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#执行收缩\n",
    "result = network.contract(slices=slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99b5ee-ab50-41f8-803a-2be5069067e2",
   "metadata": {},
   "source": [
    "使用 contract 方法进行张量收缩，仅计算当前进程负责的切片范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3323d-1118-41aa-b1e9-f68dbe6fb427",
   "metadata": {},
   "source": [
    "12) 汇总结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca371838-ce16-4eac-b54a-3daec0a1b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#汇总结果\n",
    "result = comm.reduce(\n",
    "    sendobj=result, op=MPI.SUM, root=root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774ba96-41d7-4db3-9ae9-0ccc9253e33d",
   "metadata": {},
   "source": [
    "使用 MPI.SUM 将所有进程的计算结果汇总到 root 进程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c2605-9e33-4920-846a-71e67eee8349",
   "metadata": {},
   "source": [
    "13) 检查结果正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfad412-589c-4696-815e-6efa6a10dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == root:\n",
    "   result_np = np.einsum(expr, *operands, optimize=True)\n",
    "   print(\"Does the cuQuantum parallel contraction result match the numpy.einsum result?\", np.allclose(result, result_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e56956-61a3-4e01-87a5-dbb758c159fa",
   "metadata": {},
   "source": [
    "root 进程使用 numpy.einsum 检查并行收缩的结果是否与序列化计算结果一致，以验证正确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae220a-1446-42b8-b4ee-38b991dbeae9",
   "metadata": {},
   "source": [
    "## 1.3资源分配 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08aa6e4-3cf4-4d28-b95e-f54fe5108401",
   "metadata": {},
   "source": [
    "本示例展示了如何管理有状态对象使用的内存资源。当张量网络需要大量内存，并且对有状态对象的执行方法（例如自动调优、收缩和梯度计算）的调用与对其他操作（包括对其他张量网络的操作）的调用交错进行时，这种管理非常有用，这些操作也可能需要大量内存。\n",
    "\n",
    "在本示例中，我们使用两个张量网络，分别表示两个大型矩阵乘法，并以交替的方式在循环中执行这两个收缩操作。我们假设可用的设备内存仅足够容纳一个操作数集和一次收缩操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae8e0d-82b6-45b5-bb53-bfd936e8aa01",
   "metadata": {},
   "source": [
    "1) 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20f1ec63-8f80-4715-9b89-3f54dfab2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import cuquantum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedec62-1cb0-47eb-affa-8bfdca8fb4b8",
   "metadata": {},
   "source": [
    "2) 配置和初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00f19241-6c54-467e-a84d-75bf45ffa65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)-8s %(message)s', datefmt='%m-%d %H:%M:%S')\n",
    "N = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8756b-c515-43e5-88da-a0081b104e8f",
   "metadata": {},
   "source": [
    "开启日志：用于监控内存管理消息，例如内存分配、释放的详细信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23cf60-b910-4dda-af6c-2e644d351975",
   "metadata": {},
   "source": [
    "3) 第一次初始化和收缩计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58f44d80-1dba-4a5e-b1e0-74bb9384f37c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-15 22:58:24 INFO     cuTensorNet version = 2.5.0\n",
      "11-15 22:58:24 INFO     Beginning network creation...\n",
      "11-15 22:58:24 INFO     The memory limit is 4.80 GiB.\n",
      "11-15 22:58:24 DEBUG    Beginning output tensor creation...\n",
      "11-15 22:58:24 DEBUG    The output tensor has been created.\n",
      "11-15 22:58:24 INFO     The network has been created.\n",
      "11-15 22:58:24 INFO     Setting user-provided path...\n",
      "11-15 22:58:24 INFO     Finished setting user-provided path.\n",
      "11-15 22:58:24 INFO     Optimizer Information:\n",
      "    Largest intermediate = 1.00 MiElements\n",
      "    Optimized cost = 2.147e+09 FLOPs\n",
      "    Path = [(0, 1)]\n",
      "    Slicing not needed.\n",
      "    Intermediate tensor mode labels = [ik]\n",
      "11-15 22:58:24 INFO     The workspace size requirements range from 24.00 MiB to 72.00 MiB.\n",
      "11-15 22:58:24 INFO     The scratch workspace size has been set to 72.00 MiB.\n",
      "11-15 22:58:24 INFO     The cache workspace size has been set to 0.00 B.\n",
      "11-15 22:58:24 DEBUG    Creating contraction plan...\n",
      "11-15 22:58:24 DEBUG    Finished creating contraction plan.\n",
      "11-15 22:58:24 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 22:58:24 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 75498752, ptr = 21718106112, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:24 DEBUG    Finished allocating device memory of size 72.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:24 DEBUG    The scratch workspace memory (device pointer = 21718106112, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 22:58:24 DEBUG    Allocating cache workspace for the tensor network computation...\n",
      "11-15 22:58:24 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 0, ptr = 0, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:24 DEBUG    Finished allocating device memory of size 0.00 B and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:24 DEBUG    The cache workspace memory (device pointer = 0, host pointer = 94415047293824) has been set in the workspace descriptor.\n",
      "11-15 22:58:24 DEBUG    Established ordering with output tensor creation event.\n",
      "11-15 22:58:24 INFO     All the available slices (1) will be contracted.\n",
      "11-15 22:58:24 INFO     Starting network contraction...\n",
      "11-15 22:58:24 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 22:58:24 INFO     The contraction took 148.293 ms to complete.\n",
      "11-15 22:58:24 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 22:58:24 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n"
     ]
    }
   ],
   "source": [
    "a = cp.random.rand(N, N)\n",
    "b = cp.random.rand(N, N)\n",
    "n1 = cuquantum.Network(\"ij,jk\", a, b)\n",
    "n1.contract_path()\n",
    "r = n1.contract(release_workspace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d31334-03a4-4013-af3e-a3ec71e62a97",
   "metadata": {},
   "source": [
    "张量网络 1 (n1)：\\\n",
    "定义张量网络收缩表达式 ij,jk。\\\n",
    "创建随机张量 a 和 b。\\\n",
    "创建 Network 对象 n1，并绑定操作数。\\\n",
    "路径优化：n1.contract_path() 计算收缩路径。\\\n",
    "执行收缩：n1.contract()，并通过 release_workspace=True 释放中间计算的临时内存，只保留结果 r。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdcf2e5-e9f7-4eb8-9ed7-f12f238fd4f4",
   "metadata": {},
   "source": [
    "4) 释放资源并初始化第二个网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4ec8d48-ee85-42d9-94a7-7196d202fc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-15 22:58:26 INFO     The operands have been reset to None.\n",
      "11-15 22:58:26 INFO     cuTensorNet version = 2.5.0\n",
      "11-15 22:58:26 INFO     Beginning network creation...\n",
      "11-15 22:58:26 INFO     The memory limit is 4.80 GiB.\n",
      "11-15 22:58:26 DEBUG    Beginning output tensor creation...\n",
      "11-15 22:58:26 DEBUG    The output tensor has been created.\n",
      "11-15 22:58:26 INFO     The network has been created.\n",
      "11-15 22:58:26 INFO     Setting user-provided path...\n",
      "11-15 22:58:26 INFO     Finished setting user-provided path.\n",
      "11-15 22:58:26 INFO     Optimizer Information:\n",
      "    Largest intermediate = 1.00 MiElements\n",
      "    Optimized cost = 2.147e+09 FLOPs\n",
      "    Path = [(0, 1)]\n",
      "    Slicing not needed.\n",
      "    Intermediate tensor mode labels = [ik]\n",
      "11-15 22:58:26 INFO     The workspace size requirements range from 24.00 MiB to 72.00 MiB.\n",
      "11-15 22:58:26 INFO     The scratch workspace size has been set to 72.00 MiB.\n",
      "11-15 22:58:26 INFO     The cache workspace size has been set to 0.00 B.\n",
      "11-15 22:58:26 DEBUG    Creating contraction plan...\n",
      "11-15 22:58:26 DEBUG    Finished creating contraction plan.\n",
      "11-15 22:58:26 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 22:58:26 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 75498752, ptr = 21718106112, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:26 DEBUG    Finished allocating device memory of size 72.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:26 DEBUG    The scratch workspace memory (device pointer = 21718106112, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 22:58:26 DEBUG    Allocating cache workspace for the tensor network computation...\n",
      "11-15 22:58:26 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 0, ptr = 0, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:26 DEBUG    Finished allocating device memory of size 0.00 B and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:26 DEBUG    The cache workspace memory (device pointer = 0, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 22:58:26 DEBUG    Established ordering with output tensor creation event.\n",
      "11-15 22:58:26 INFO     All the available slices (1) will be contracted.\n",
      "11-15 22:58:26 INFO     Starting network contraction...\n",
      "11-15 22:58:26 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 22:58:27 INFO     The contraction took 148.291 ms to complete.\n",
      "11-15 22:58:27 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 22:58:27 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n"
     ]
    }
   ],
   "source": [
    "n1.reset_operands(None)\n",
    "a = b = None\n",
    "\n",
    "c = cp.random.rand(N, N)\n",
    "d = cp.random.rand(N, N)\n",
    "n2 = cuquantum.Network(\"ij,jk\", c, d)\n",
    "n2.contract_path()\n",
    "r = n2.contract(release_workspace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffafb877-d2c7-4acd-ab8b-5aef8c17897d",
   "metadata": {},
   "source": [
    "释放资源：\\\n",
    "调用 n1.reset_operands(None) 释放 n1 的操作数 a 和 b 的内存。\\\n",
    "将变量 a 和 b 设为 None，方便后续重新绑定。\\\n",
    "张量网络 2 (n2)：\\\n",
    "创建随机张量 c 和 d。\\\n",
    "创建 Network 对象 n2，并绑定操作数。\\\n",
    "路径优化：n2.contract_path() 计算收缩路径。\\\n",
    "执行收缩：通过 release_workspace=True 释放中间计算的临时内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff90f6f-660f-41f8-bdd6-829a3e9a5f20",
   "metadata": {},
   "source": [
    "5) 交替收缩循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16ea497f-e831-4858-a1f4-0a328b9c266b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-15 22:58:28 INFO     Resetting operands...\n",
      "11-15 22:58:28 INFO     The operands have been reset.\n",
      "11-15 22:58:28 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 22:58:28 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 75498752, ptr = 21718106112, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:28 DEBUG    Finished allocating device memory of size 72.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:28 DEBUG    The scratch workspace memory (device pointer = 21718106112, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 22:58:28 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 22:58:28 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 22:58:28 INFO     All the available slices (1) will be contracted.\n",
      "11-15 22:58:28 INFO     Starting network contraction...\n",
      "11-15 22:58:28 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 22:58:28 INFO     The contraction took 76.157 ms to complete.\n",
      "11-15 22:58:28 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 22:58:28 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 22:58:28 INFO     The operands have been reset to None.\n",
      "11-15 22:58:28 INFO     Resetting operands...\n",
      "11-15 22:58:28 INFO     The operands have been reset.\n",
      "11-15 22:58:28 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 22:58:28 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 75498752, ptr = 21718106112, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:28 DEBUG    Finished allocating device memory of size 72.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:28 DEBUG    The scratch workspace memory (device pointer = 21718106112, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 22:58:28 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 22:58:28 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 22:58:28 INFO     All the available slices (1) will be contracted.\n",
      "11-15 22:58:28 INFO     Starting network contraction...\n",
      "11-15 22:58:28 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 22:58:28 INFO     The contraction took 76.081 ms to complete.\n",
      "11-15 22:58:28 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 22:58:28 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 22:58:28 INFO     The operands have been reset to None.\n",
      "11-15 22:58:28 INFO     Resetting operands...\n",
      "11-15 22:58:28 INFO     The operands have been reset.\n",
      "11-15 22:58:28 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 22:58:28 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 75498752, ptr = 21718106112, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:28 DEBUG    Finished allocating device memory of size 72.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:28 DEBUG    The scratch workspace memory (device pointer = 21718106112, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 22:58:28 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 22:58:28 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 22:58:28 INFO     All the available slices (1) will be contracted.\n",
      "11-15 22:58:28 INFO     Starting network contraction...\n",
      "11-15 22:58:28 INFO     This call is blocking and will return only after the operation is complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-15 22:58:28 INFO     The contraction took 66.218 ms to complete.\n",
      "11-15 22:58:28 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 22:58:28 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 22:58:28 INFO     The operands have been reset to None.\n",
      "11-15 22:58:28 INFO     Resetting operands...\n",
      "11-15 22:58:28 INFO     The operands have been reset.\n",
      "11-15 22:58:28 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 22:58:28 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 75498752, ptr = 21718106112, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:28 DEBUG    Finished allocating device memory of size 72.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:28 DEBUG    The scratch workspace memory (device pointer = 21718106112, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 22:58:28 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 22:58:28 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 22:58:28 INFO     All the available slices (1) will be contracted.\n",
      "11-15 22:58:28 INFO     Starting network contraction...\n",
      "11-15 22:58:28 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 22:58:28 INFO     The contraction took 15.157 ms to complete.\n",
      "11-15 22:58:28 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 22:58:28 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 22:58:28 INFO     The operands have been reset to None.\n",
      "11-15 22:58:28 INFO     Resetting operands...\n",
      "11-15 22:58:28 INFO     The operands have been reset.\n",
      "11-15 22:58:28 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 22:58:28 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 75498752, ptr = 21718106112, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:28 DEBUG    Finished allocating device memory of size 72.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:28 DEBUG    The scratch workspace memory (device pointer = 21718106112, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 22:58:28 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 22:58:28 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 22:58:28 INFO     All the available slices (1) will be contracted.\n",
      "11-15 22:58:28 INFO     Starting network contraction...\n",
      "11-15 22:58:28 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 22:58:28 INFO     The contraction took 15.294 ms to complete.\n",
      "11-15 22:58:28 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 22:58:28 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 22:58:28 INFO     The operands have been reset to None.\n",
      "11-15 22:58:28 INFO     Resetting operands...\n",
      "11-15 22:58:28 INFO     The operands have been reset.\n",
      "11-15 22:58:28 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 22:58:28 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 75498752, ptr = 21718106112, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 22:58:28 DEBUG    Finished allocating device memory of size 72.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 22:58:28 DEBUG    The scratch workspace memory (device pointer = 21718106112, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 22:58:28 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 22:58:28 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 22:58:28 INFO     All the available slices (1) will be contracted.\n",
      "11-15 22:58:28 INFO     Starting network contraction...\n",
      "11-15 22:58:28 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 22:58:28 INFO     The contraction took 15.264 ms to complete.\n",
      "11-15 22:58:28 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 22:58:28 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 22:58:28 INFO     The operands have been reset to None.\n",
      "11-15 22:58:28 DEBUG    [_free_workspace_memory] The scratch and cache workspace memory has been released.\n",
      "11-15 22:58:28 INFO     The network resources have been released.\n",
      "11-15 22:58:28 DEBUG    [_free_workspace_memory] The scratch and cache workspace memory has been released.\n",
      "11-15 22:58:28 INFO     The network resources have been released.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\n"
     ]
    }
   ],
   "source": [
    "with n1, n2:\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        print(f\"Iteration {i}\")\n",
    "        # Create and set new operands for n1.\n",
    "        a = cp.random.rand(N, N)\n",
    "        b = cp.random.rand(N, N)\n",
    "        n1.reset_operands(a, b)\n",
    "\n",
    "        # Perform the first contraction\n",
    "        r = n1.contract(release_workspace=True)\n",
    "\n",
    "        # Reset network n1 operands\n",
    "        n1.reset_operands(None)\n",
    "        a = b = None\n",
    "\n",
    "        # Create and set new operands for n2\n",
    "        c = cp.random.rand(N, N)\n",
    "        d = cp.random.rand(N, N)\n",
    "        n2.reset_operands(c, d)\n",
    "\n",
    "        # Perform the second contraction\n",
    "        r = n2.contract(release_workspace=True)\n",
    "\n",
    "        # Reset network n2 operands\n",
    "        n2.reset_operands(None)\n",
    "        c = d = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095f509-bf10-470a-aa2f-401880f5c53c",
   "metadata": {},
   "source": [
    "使用 with 上下文管理器，确保在退出时自动释放 n1 和 n2 的所有资源。\\\n",
    "在每次迭代中：\\\n",
    "绑定新数据：生成随机张量 a, b, c, d，分别绑定到 n1 和 n2。\\\n",
    "执行收缩：\\\n",
    "使用 n1.contract() 和 n2.contract() 分别进行张量收缩。\\\n",
    "每次操作后通过 release_workspace=True 释放工作区内存。\\\n",
    "释放操作数：\\\n",
    "调用 reset_operands(None) 释放当前操作数。\\\n",
    "循环交替进行两个网络的收缩。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302ef33-3f87-49ca-a8d0-d031430d441e",
   "metadata": {},
   "source": [
    "6) 总代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a56af73-0750-4aca-b769-e1b02dc57286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-15 23:03:40 INFO     cuTensorNet version = 2.5.0\n",
      "11-15 23:03:40 INFO     Beginning network creation...\n",
      "11-15 23:03:40 INFO     The memory limit is 4.80 GiB.\n",
      "11-15 23:03:40 DEBUG    Beginning output tensor creation...\n",
      "11-15 23:03:40 DEBUG    The output tensor has been created.\n",
      "11-15 23:03:40 INFO     The network has been created.\n",
      "11-15 23:03:40 INFO     Setting user-provided path...\n",
      "11-15 23:03:40 INFO     Finished setting user-provided path.\n",
      "11-15 23:03:40 INFO     Optimizer Information:\n",
      "    Largest intermediate = 64.00 MiElements\n",
      "    Optimized cost = 1.100e+12 FLOPs\n",
      "    Path = [(0, 1)]\n",
      "    Slicing not needed.\n",
      "    Intermediate tensor mode labels = [ik]\n",
      "11-15 23:03:40 INFO     The workspace size requirements range from 528.00 MiB to 576.00 MiB.\n",
      "11-15 23:03:40 INFO     The scratch workspace size has been set to 576.00 MiB.\n",
      "11-15 23:03:40 INFO     The cache workspace size has been set to 0.00 B.\n",
      "11-15 23:03:40 DEBUG    Creating contraction plan...\n",
      "11-15 23:03:40 DEBUG    Finished creating contraction plan.\n",
      "11-15 23:03:40 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 23:03:40 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 603981056, ptr = 22791847936, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:03:40 DEBUG    Finished allocating device memory of size 576.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:03:40 DEBUG    The scratch workspace memory (device pointer = 22791847936, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 23:03:40 DEBUG    Allocating cache workspace for the tensor network computation...\n",
      "11-15 23:03:40 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 0, ptr = 0, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:03:40 DEBUG    Finished allocating device memory of size 0.00 B and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:03:40 DEBUG    The cache workspace memory (device pointer = 0, host pointer = 94415127363760) has been set in the workspace descriptor.\n",
      "11-15 23:03:40 DEBUG    Established ordering with output tensor creation event.\n",
      "11-15 23:03:40 INFO     All the available slices (1) will be contracted.\n",
      "11-15 23:03:40 INFO     Starting network contraction...\n",
      "11-15 23:03:40 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 23:03:47 INFO     The contraction took 5993.951 ms to complete.\n",
      "11-15 23:03:47 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 23:03:47 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 23:03:47 INFO     The operands have been reset to None.\n",
      "11-15 23:03:47 INFO     cuTensorNet version = 2.5.0\n",
      "11-15 23:03:47 INFO     Beginning network creation...\n",
      "11-15 23:03:47 INFO     The memory limit is 4.80 GiB.\n",
      "11-15 23:03:47 DEBUG    Beginning output tensor creation...\n",
      "11-15 23:03:47 DEBUG    The output tensor has been created.\n",
      "11-15 23:03:47 INFO     The network has been created.\n",
      "11-15 23:03:47 INFO     Setting user-provided path...\n",
      "11-15 23:03:47 INFO     Finished setting user-provided path.\n",
      "11-15 23:03:47 INFO     Optimizer Information:\n",
      "    Largest intermediate = 64.00 MiElements\n",
      "    Optimized cost = 1.100e+12 FLOPs\n",
      "    Path = [(0, 1)]\n",
      "    Slicing not needed.\n",
      "    Intermediate tensor mode labels = [ik]\n",
      "11-15 23:03:47 INFO     The workspace size requirements range from 528.00 MiB to 576.00 MiB.\n",
      "11-15 23:03:47 INFO     The scratch workspace size has been set to 576.00 MiB.\n",
      "11-15 23:03:47 INFO     The cache workspace size has been set to 0.00 B.\n",
      "11-15 23:03:47 DEBUG    Creating contraction plan...\n",
      "11-15 23:03:47 DEBUG    Finished creating contraction plan.\n",
      "11-15 23:03:47 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 23:03:47 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 603981056, ptr = 23328718848, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:03:47 DEBUG    Finished allocating device memory of size 576.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:03:47 DEBUG    The scratch workspace memory (device pointer = 23328718848, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 23:03:47 DEBUG    Allocating cache workspace for the tensor network computation...\n",
      "11-15 23:03:47 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 0, ptr = 0, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:03:47 DEBUG    Finished allocating device memory of size 0.00 B and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:03:47 DEBUG    The cache workspace memory (device pointer = 0, host pointer = 94415047293824) has been set in the workspace descriptor.\n",
      "11-15 23:03:47 DEBUG    Established ordering with output tensor creation event.\n",
      "11-15 23:03:47 INFO     All the available slices (1) will be contracted.\n",
      "11-15 23:03:47 INFO     Starting network contraction...\n",
      "11-15 23:03:47 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 23:03:53 INFO     The contraction took 5968.672 ms to complete.\n",
      "11-15 23:03:53 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 23:03:53 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 23:03:53 INFO     The operands have been reset to None.\n",
      "11-15 23:03:53 INFO     Resetting operands...\n",
      "11-15 23:03:53 INFO     The operands have been reset.\n",
      "11-15 23:03:53 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 23:03:53 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 603981056, ptr = 23328718848, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:03:53 DEBUG    Finished allocating device memory of size 576.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:03:53 DEBUG    The scratch workspace memory (device pointer = 23328718848, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 23:03:53 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 23:03:53 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 23:03:53 INFO     All the available slices (1) will be contracted.\n",
      "11-15 23:03:53 INFO     Starting network contraction...\n",
      "11-15 23:03:53 INFO     This call is blocking and will return only after the operation is complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-15 23:03:59 INFO     The contraction took 6006.506 ms to complete.\n",
      "11-15 23:03:59 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 23:03:59 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 23:03:59 INFO     The operands have been reset to None.\n",
      "11-15 23:03:59 INFO     Resetting operands...\n",
      "11-15 23:03:59 INFO     The operands have been reset.\n",
      "11-15 23:03:59 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 23:03:59 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 603981056, ptr = 22791847936, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:03:59 DEBUG    Finished allocating device memory of size 576.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:03:59 DEBUG    The scratch workspace memory (device pointer = 22791847936, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 23:03:59 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 23:03:59 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 23:03:59 INFO     All the available slices (1) will be contracted.\n",
      "11-15 23:03:59 INFO     Starting network contraction...\n",
      "11-15 23:03:59 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 23:04:06 INFO     The contraction took 6055.834 ms to complete.\n",
      "11-15 23:04:06 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 23:04:06 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 23:04:06 INFO     The operands have been reset to None.\n",
      "11-15 23:04:06 INFO     Resetting operands...\n",
      "11-15 23:04:06 INFO     The operands have been reset.\n",
      "11-15 23:04:06 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 23:04:06 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 603981056, ptr = 22254977024, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:04:06 DEBUG    Finished allocating device memory of size 576.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:04:06 DEBUG    The scratch workspace memory (device pointer = 22254977024, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 23:04:06 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 23:04:06 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 23:04:06 INFO     All the available slices (1) will be contracted.\n",
      "11-15 23:04:06 INFO     Starting network contraction...\n",
      "11-15 23:04:06 INFO     This call is blocking and will return only after the operation is complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-15 23:04:12 INFO     The contraction took 6014.287 ms to complete.\n",
      "11-15 23:04:12 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 23:04:12 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 23:04:12 INFO     The operands have been reset to None.\n",
      "11-15 23:04:12 INFO     Resetting operands...\n",
      "11-15 23:04:12 INFO     The operands have been reset.\n",
      "11-15 23:04:12 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 23:04:12 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 603981056, ptr = 22254977024, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:04:12 DEBUG    Finished allocating device memory of size 576.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:04:12 DEBUG    The scratch workspace memory (device pointer = 22254977024, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 23:04:12 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 23:04:12 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 23:04:12 INFO     All the available slices (1) will be contracted.\n",
      "11-15 23:04:12 INFO     Starting network contraction...\n",
      "11-15 23:04:12 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 23:04:18 INFO     The contraction took 6039.229 ms to complete.\n",
      "11-15 23:04:18 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 23:04:18 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 23:04:18 INFO     The operands have been reset to None.\n",
      "11-15 23:04:18 INFO     Resetting operands...\n",
      "11-15 23:04:18 INFO     The operands have been reset.\n",
      "11-15 23:04:18 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 23:04:18 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 603981056, ptr = 22254977024, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:04:18 DEBUG    Finished allocating device memory of size 576.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:04:18 DEBUG    The scratch workspace memory (device pointer = 22254977024, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 23:04:18 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 23:04:18 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 23:04:18 INFO     All the available slices (1) will be contracted.\n",
      "11-15 23:04:18 INFO     Starting network contraction...\n",
      "11-15 23:04:18 INFO     This call is blocking and will return only after the operation is complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-15 23:04:24 INFO     The contraction took 5997.393 ms to complete.\n",
      "11-15 23:04:24 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 23:04:24 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 23:04:24 INFO     The operands have been reset to None.\n",
      "11-15 23:04:24 INFO     Resetting operands...\n",
      "11-15 23:04:24 INFO     The operands have been reset.\n",
      "11-15 23:04:24 DEBUG    Allocating scratch workspace for the tensor network computation...\n",
      "11-15 23:04:24 DEBUG    _CupyCUDAMemoryManager (allocate memory): size = 603981056, ptr = 22254977024, device = 0, stream=<Stream 0 (device -1)>\n",
      "11-15 23:04:24 DEBUG    Finished allocating device memory of size 576.00 MiB and host memory of size 0.00 B for contraction in the context of stream <ExternalStream 0 (device -1)>.\n",
      "11-15 23:04:24 DEBUG    The scratch workspace memory (device pointer = 22254977024, host pointer = 94415120035424) has been set in the workspace descriptor.\n",
      "11-15 23:04:24 DEBUG    Beginning output (empty) tensor creation...\n",
      "11-15 23:04:24 DEBUG    The output (empty) tensor has been created.\n",
      "11-15 23:04:24 INFO     All the available slices (1) will be contracted.\n",
      "11-15 23:04:24 INFO     Starting network contraction...\n",
      "11-15 23:04:24 INFO     This call is blocking and will return only after the operation is complete.\n",
      "11-15 23:04:30 DEBUG    [_release_scratch_memory_perhaps] The release_workspace flag is set to True based upon the value of 'workspace_scratch_allocated_here'.\n",
      "11-15 23:04:30 DEBUG    Established ordering with respect to the computation before releasing the scratch workspace.\n",
      "11-15 23:04:30 DEBUG    [_release_workspace_memory_perhaps] The scratch workspace memory has been released.\n",
      "11-15 23:04:30 DEBUG    [_release_cache_memory_perhaps] The release_workspace flag is set to False based upon the value of 'workspace_cache_allocated_here'.\n",
      "11-15 23:04:30 DEBUG    [_free_workspace_memory] The scratch and cache workspace memory has been released.\n",
      "11-15 23:04:30 INFO     The network resources have been released.\n",
      "11-15 23:04:30 DEBUG    [_free_workspace_memory] The scratch and cache workspace memory has been released.\n",
      "11-15 23:04:30 INFO     The network resources have been released.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m n2\u001b[38;5;241m.\u001b[39mreset_operands(c, d)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Perform the second contraction, and request that the workspace be released at the end of the operation so that there is enough\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#   memory for the next iteration of the first contraction.\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m r \u001b[38;5;241m=\u001b[39m n2\u001b[38;5;241m.\u001b[39mcontract(release_workspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Reset network n2 operands to None, and set c and d to None to make memory available for next contraction and operands of network n1.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m n2\u001b[38;5;241m.\u001b[39mreset_operands(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/_internal/utils.py:464\u001b[0m, in \u001b[0;36mprecondition.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03mCheck preconditions and if they are met, call the wrapped function.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m checker(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, what\u001b[38;5;241m=\u001b[39mwhat)\n\u001b[0;32m--> 464\u001b[0m result \u001b[38;5;241m=\u001b[39m wrapped_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/_internal/utils.py:464\u001b[0m, in \u001b[0;36mprecondition.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03mCheck preconditions and if they are met, call the wrapped function.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m checker(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, what\u001b[38;5;241m=\u001b[39mwhat)\n\u001b[0;32m--> 464\u001b[0m result \u001b[38;5;241m=\u001b[39m wrapped_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "    \u001b[0;31m[... skipping similar frames: precondition.<locals>.outer.<locals>.inner at line 464 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/_internal/utils.py:464\u001b[0m, in \u001b[0;36mprecondition.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;124;03mCheck preconditions and if they are met, call the wrapped function.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m checker(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, what\u001b[38;5;241m=\u001b[39mwhat)\n\u001b[0;32m--> 464\u001b[0m result \u001b[38;5;241m=\u001b[39m wrapped_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/_internal/utils.py:433\u001b[0m, in \u001b[0;36matomic.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         flag \u001b[38;5;241m=\u001b[39m handler(e)\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flag:\n\u001b[0;32m--> 433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/_internal/utils.py:425\u001b[0m, in \u001b[0;36matomic.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03mCall the wrapped function and return the result. If an exception occurs, then call the exception handler and\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03mreraise the exception.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     result \u001b[38;5;241m=\u001b[39m wrapped_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/_internal/utils.py:433\u001b[0m, in \u001b[0;36matomic.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m         flag \u001b[38;5;241m=\u001b[39m handler(e)\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flag:\n\u001b[0;32m--> 433\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/_internal/utils.py:425\u001b[0m, in \u001b[0;36matomic.<locals>.outer.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03mCall the wrapped function and return the result. If an exception occurs, then call the exception handler and\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03mreraise the exception.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     result \u001b[38;5;241m=\u001b[39m wrapped_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/tensor_network.py:1044\u001b[0m, in \u001b[0;36mNetwork.contract\u001b[0;34m(self, slices, stream, release_workspace)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting network contraction...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_prologue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mdevice_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_id), utils\u001b[38;5;241m.\u001b[39mcuda_call_ctx(stream_holder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocking, timing) \u001b[38;5;28;01mas\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_compute_event, elapsed):\n\u001b[1;32m   1045\u001b[0m     cutn\u001b[38;5;241m.\u001b[39mcontract_slices(\n\u001b[1;32m   1046\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplan, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontraction\u001b[38;5;241m.\u001b[39mdata_ptr, \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1047\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkspace_desc, slice_group, stream_holder\u001b[38;5;241m.\u001b[39mptr)\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elapsed\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/cuquantum/cutensornet/_internal/utils.py:391\u001b[0m, in \u001b[0;36mcuda_call_ctx\u001b[0;34m(stream_holder, blocking, timing)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blocking:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m end\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timing:\n\u001b[1;32m    394\u001b[0m     time\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_elapsed_time(start, end)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "N=2**13\n",
    "# Turn on logging and set the level to DEBUG to print memory management messages.\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)-8s %(message)s', datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "# Create, prepare, and execute the first iteration on network n1.\n",
    "a = cp.random.rand(N, N)\n",
    "b = cp.random.rand(N, N)\n",
    "n1 = cuquantum.Network(\"ij,jk\", a, b)\n",
    "n1.contract_path()\n",
    "r = n1.contract(release_workspace=True)\n",
    "\n",
    "# Reset network n1 operands to None, and set a and b to None to make memory available for the network n2.\n",
    "n1.reset_operands(None)\n",
    "a = b = None\n",
    "\n",
    "# Create, prepare, and execute the first iteration on network n2.\n",
    "c = cp.random.rand(N, N)\n",
    "d = cp.random.rand(N, N)\n",
    "n2 = cuquantum.Network(\"ij,jk\", c, d)\n",
    "n2.contract_path()\n",
    "r = n2.contract(release_workspace=True)\n",
    "\n",
    "# Reset network n2 operands to None, and set c and d to None to make memory available for next contraction of network n1.\n",
    "n2.reset_operands(None)\n",
    "c = d = None\n",
    "\n",
    "num_iter = 3\n",
    "# Use the networks as context managers so that internal library resources are properly cleaned up.\n",
    "with n1, n2:\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        print(f\"Iteration {i}\")\n",
    "        # Create and set new operands for n1.\n",
    "        a = cp.random.rand(N, N)\n",
    "        b = cp.random.rand(N, N)\n",
    "        n1.reset_operands(a, b)\n",
    "\n",
    "        # Perform the first contraction, and request that the workspace be released at the end of the operation so that there is enough\n",
    "        #   memory for the second one.\n",
    "        r = n1.contract(release_workspace=True)\n",
    "\n",
    "        # Reset network n1 operands to None, and set a and b to None to make memory available for the operands for and contracting network n2.\n",
    "        n1.reset_operands(None)\n",
    "        a = b = None\n",
    "\n",
    "        # Create and set new operands for n2.\n",
    "        c = cp.random.rand(N, N)\n",
    "        d = cp.random.rand(N, N)\n",
    "        n2.reset_operands(c, d)\n",
    "\n",
    "        # Perform the second contraction, and request that the workspace be released at the end of the operation so that there is enough\n",
    "        #   memory for the next iteration of the first contraction.\n",
    "        r = n2.contract(release_workspace=True)\n",
    "\n",
    "        # Reset network n2 operands to None, and set c and d to None to make memory available for next contraction and operands of network n1.\n",
    "        n2.reset_operands(None)\n",
    "        c = d = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4b224-80e2-4da1-87d8-189bf9656350",
   "metadata": {},
   "source": [
    "# 2.Network类与张量分解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad2ca3-1241-408b-bb79-9ea78b24937c",
   "metadata": {},
   "source": [
    "![jupyter](./1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605987a-28f8-4e1e-b126-86ace13a051c",
   "metadata": {},
   "source": [
    "1) 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9426569-4dc6-478c-b9d8-48298fb62043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "from cuquantum import cutensornet as cutn\n",
    "from cuquantum.cutensornet.experimental import contract_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7876e58-2abb-4abc-95ec-566b2e33fa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 14:25:54,032 [INFO] cuTensorNet version = 2.5.0\n",
      "2024-11-25 14:25:54,034 [INFO] Beginning operands parsing...\n",
      "2024-11-25 14:25:54,036 [INFO] Calling specicialized kernel `cutensornetGateSplit` for contraction and decomposition.\n",
      "2024-11-25 14:25:54,041 [INFO] The SVDConfig attribute 'abs_cutoff' has been set to 1e-25.\n",
      "2024-11-25 14:25:54,041 [INFO] The SVDConfig attribute 'rel_cutoff' has been set to 0.0.\n",
      "2024-11-25 14:25:54,043 [INFO] The SVDConfig attribute 'partition' has been set to 2.\n",
      "2024-11-25 14:25:54,044 [INFO] The SVDConfig attribute 'normalization' has been set to 0.\n",
      "2024-11-25 14:25:54,046 [INFO] The SVDConfig attribute 'algorithm' has been set to 0.\n",
      "2024-11-25 14:25:54,046 [INFO] The SVDConfig attribute 'discarded_weight_cutoff' has been set to 0.0.\n",
      "2024-11-25 14:25:54,091 [INFO] Starting contract-decompose (gate split)...\n",
      "2024-11-25 14:25:54,092 [INFO] This call is blocking and will return only after the operation is complete.\n",
      "2024-11-25 14:25:54,267 [INFO] The contract-decompose (gate split) operation took 173.217 ms to complete.\n",
      "2024-11-25 14:25:54,270 [INFO] cuTensorNet version = 2.5.0\n",
      "2024-11-25 14:25:54,271 [INFO] Beginning operands parsing...\n",
      "2024-11-25 14:25:54,272 [INFO] Calling specicialized kernel `cutensornetGateSplit` for contraction and decomposition.\n",
      "2024-11-25 14:25:54,273 [INFO] The SVDConfig attribute 'abs_cutoff' has been set to 1e-25.\n",
      "2024-11-25 14:25:54,274 [INFO] The SVDConfig attribute 'rel_cutoff' has been set to 0.0.\n",
      "2024-11-25 14:25:54,275 [INFO] The SVDConfig attribute 'partition' has been set to 2.\n",
      "2024-11-25 14:25:54,277 [INFO] The SVDConfig attribute 'normalization' has been set to 0.\n",
      "2024-11-25 14:25:54,277 [INFO] The SVDConfig attribute 'algorithm' has been set to 0.\n",
      "2024-11-25 14:25:54,278 [INFO] The SVDConfig attribute 'discarded_weight_cutoff' has been set to 0.0.\n",
      "2024-11-25 14:25:54,280 [INFO] Starting contract-decompose (gate split)...\n",
      "2024-11-25 14:25:54,281 [INFO] This call is blocking and will return only after the operation is complete.\n",
      "2024-11-25 14:25:54,285 [INFO] The contract-decompose (gate split) operation took 2.706 ms to complete.\n",
      "2024-11-25 14:25:54,287 [INFO] cuTensorNet version = 2.5.0\n",
      "2024-11-25 14:25:54,288 [INFO] Beginning operands parsing...\n",
      "2024-11-25 14:25:54,289 [INFO] Calling specicialized kernel `cutensornetGateSplit` for contraction and decomposition.\n",
      "2024-11-25 14:25:54,291 [INFO] The SVDConfig attribute 'abs_cutoff' has been set to 1e-12.\n",
      "2024-11-25 14:25:54,291 [INFO] The SVDConfig attribute 'rel_cutoff' has been set to 0.0.\n",
      "2024-11-25 14:25:54,292 [INFO] The SVDConfig attribute 'partition' has been set to 2.\n",
      "2024-11-25 14:25:54,294 [INFO] The SVDConfig attribute 'normalization' has been set to 0.\n",
      "2024-11-25 14:25:54,295 [INFO] The SVDConfig attribute 'algorithm' has been set to 0.\n",
      "2024-11-25 14:25:54,295 [INFO] The SVDConfig attribute 'discarded_weight_cutoff' has been set to 0.0.\n",
      "2024-11-25 14:25:54,297 [INFO] Starting contract-decompose (gate split)...\n",
      "2024-11-25 14:25:54,298 [INFO] This call is blocking and will return only after the operation is complete.\n",
      "2024-11-25 14:25:54,302 [INFO] The contract-decompose (gate split) operation took 2.143 ms to complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "最终MPS张量的形状：\n",
      "站点 0，形状: (1, 2, 1)\n",
      "站点 1，形状: (1, 2, 1)\n",
      "站点 2，形状: (1, 2, 1)\n",
      "站点 3，形状: (1, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义初始MPS生成函数\n",
    "def generate_initial_mps(n_qubits, dtype='complex128'):\n",
    "    \"\"\"生成初始MPS（|000...0>）\"\"\"\n",
    "    state_tensor = cp.asarray([1, 0], dtype=dtype).reshape(1, 2, 1)\n",
    "    return [state_tensor.copy() for _ in range(n_qubits)]\n",
    "\n",
    "# 定义双量子比特门作用函数\n",
    "def apply_two_qubit_gate(mps, gate, qubits, cutoff=1e-12, handle=None):\n",
    "    \"\"\"\n",
    "    对MPS作用双量子比特门，并进行截断\n",
    "    Args:\n",
    "        mps: 当前的MPS张量列表\n",
    "        gate: 4维双量子比特门张量\n",
    "        qubits: [q1, q2] 要作用的量子比特\n",
    "        cutoff: 截断阈值\n",
    "        handle: cuQuantum的上下文句柄\n",
    "    Returns:\n",
    "        更新后的MPS张量列表\n",
    "    \"\"\"\n",
    "    i, j = qubits\n",
    "    if abs(i - j) != 1:\n",
    "        raise ValueError(\"仅支持相邻双量子比特门\")\n",
    "    # 合并两个站点的张量并作用门\n",
    "    combined_tensor, _, truncated_tensor = contract_decompose(\n",
    "        'ipj,jqk,rspq->irj,jsk',\n",
    "        mps[i], mps[j], gate,\n",
    "        algorithm={'svd_method': {'partition': 'V', 'abs_cutoff': cutoff}},\n",
    "        options={'handle': handle}  # 使用句柄\n",
    "    )\n",
    "    print(_)\n",
    "    mps[i], mps[j] = combined_tensor, truncated_tensor\n",
    "    return mps\n",
    "\n",
    "# 初始化MPS\n",
    "n_qubits = 4\n",
    "dtype = 'complex128'\n",
    "mps = generate_initial_mps(n_qubits, dtype=dtype)\n",
    "\n",
    "# 初始化双量子比特门（CNOT门为例）\n",
    "cnot_gate = cp.zeros((2, 2, 2, 2), dtype=dtype)\n",
    "cnot_gate[0, 0, 0, 0] = cnot_gate[1, 1, 1, 1] = cnot_gate[1, 0, 1, 0] = cnot_gate[0, 1, 1, 0] = 1\n",
    "\n",
    "# 定义cuQuantum句柄\n",
    "handle = cutn.create()\n",
    "\n",
    "# 对MPS作用CNOT门并截断\n",
    "mps = apply_two_qubit_gate(mps, cnot_gate, qubits=(0, 1), cutoff=1e-25, handle=handle)\n",
    "mps = apply_two_qubit_gate(mps, cnot_gate, qubits=(1, 2), cutoff=1e-25, handle=handle)\n",
    "mps = apply_two_qubit_gate(mps, cnot_gate, qubits=(2, 3), cutoff=1e-12, handle=handle)\n",
    "\n",
    "# 打印结果\n",
    "print(\"最终MPS张量的形状：\")\n",
    "for idx, tensor in enumerate(mps):\n",
    "    print(f\"站点 {idx}，形状: {tensor.shape}\")\n",
    "\n",
    "# 销毁cuQuantum句柄，释放资源\n",
    "cutn.destroy(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f975e88-454d-4956-9360-bd88865a8fbf",
   "metadata": {},
   "source": [
    "![jupyter](./2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f188bbbd-7ea7-4a6c-b54d-e1661e8da3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 14:29:16,765 [INFO] CUDA runtime version = 12020\n",
      "2024-11-25 14:29:16,768 [INFO] cuTensorNet version = 2.5.0\n",
      "2024-11-25 14:29:16,769 [INFO] Beginning operands parsing...\n",
      "2024-11-25 14:29:16,771 [INFO] Begin transferring input data from host to device 0\n",
      "2024-11-25 14:29:16,773 [INFO] Input data transfer finished\n",
      "2024-11-25 14:29:16,774 [INFO] The SVDConfig attribute 'abs_cutoff' has been set to 1e-06.\n",
      "2024-11-25 14:29:16,775 [INFO] The SVDConfig attribute 'rel_cutoff' has been set to 0.01.\n",
      "2024-11-25 14:29:16,776 [INFO] The SVDConfig attribute 'partition' has been set to 2.\n",
      "2024-11-25 14:29:16,777 [INFO] The SVDConfig attribute 'normalization' has been set to 0.\n",
      "2024-11-25 14:29:16,778 [INFO] The SVDConfig attribute 'algorithm' has been set to 0.\n",
      "2024-11-25 14:29:16,778 [INFO] The SVDConfig attribute 'discarded_weight_cutoff' has been set to 0.1.\n",
      "2024-11-25 14:29:17,497 [INFO] Starting tensor decomposition...\n",
      "2024-11-25 14:29:17,498 [INFO] This call is blocking and will return only after the operation is complete.\n",
      "2024-11-25 14:29:17,521 [INFO] The SVD decomposition took 21.627 ms to complete.\n",
      "2024-11-25 14:29:17,527 [INFO] All resources for the decomposition are freed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 张量形状: (4, 3, 3)\n",
      "V 张量形状: (3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from cuquantum.cutensornet.tensor import decompose, SVDMethod\n",
    "\n",
    "def svd_decompose_with_options(expr, tensor_data, abs_cutoff=None, rel_cutoff=None, discarded_weight_cutoff=None):\n",
    "    \"\"\"\n",
    "    使用 cuTensorNet 的 SVD 方法对张量进行分解并截断，支持多种选项。\n",
    "    \n",
    "    Args:\n",
    "        expr (str): 张量分解表达式。\n",
    "        tensor_data (ndarray): 输入张量。\n",
    "        abs_cutoff (float): 绝对截断值。\n",
    "        rel_cutoff (float): 相对截断值。\n",
    "        discarded_weight_cutoff (float): 丢弃权重截断值。\n",
    "\n",
    "    Returns:\n",
    "        tuple: 分解后的张量 U, S, V。\n",
    "    \"\"\"\n",
    "    # 定义 SVD 方法\n",
    "    svd_method = SVDMethod(\n",
    "        abs_cutoff=abs_cutoff,\n",
    "        rel_cutoff=rel_cutoff,\n",
    "        discarded_weight_cutoff=discarded_weight_cutoff,\n",
    "        partition='V'\n",
    "\n",
    "    )\n",
    "\n",
    "    # 设置日志记录器\n",
    "    logger = logging.getLogger(\"cuTensorNet_SVD\")\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "    # 定义分解选项\n",
    "\n",
    "\n",
    "    # 执行分解\n",
    "    u, s, v = decompose(expr, tensor_data, method=svd_method)\n",
    "\n",
    "    return u, s, v\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 创建张量\n",
    "    t = np.random.rand(4, 3, 3, 3)\n",
    "\n",
    "    # 修正后的分解表达式\n",
    "    svd_expr = \"ijab->ijm,mab\"\n",
    "\n",
    "    # 执行 SVD 分解\n",
    "    u, s, v = svd_decompose_with_options(\n",
    "        svd_expr, \n",
    "        t, \n",
    "        abs_cutoff=1e-6, \n",
    "        rel_cutoff=0.01, \n",
    "        discarded_weight_cutoff=0.1\n",
    "    )\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"U 张量形状:\", u.shape)\n",
    "    print(\"V 张量形状:\", v.shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5be66-bc44-48da-ad5d-3e792f0c7cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c22378-d867-41ba-a991-30501ac6a6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "panel-cell-order": [
   "c4eeac15-d223-426b-8fe0-f961c321f5af",
   "6aa5c38c-ca27-4232-bd1e-e84495f93a43",
   "ac42e2a6-8dd9-4f8b-898f-57b70f6ad5fa",
   "64f75d8a-9131-471a-915a-83ea19f329e5",
   "19d9426c-9551-455c-ab41-bd432ed0ab45",
   "dc388926-9e69-4a75-8709-efc77a04a37c",
   "e069b35f-31b8-4298-8d25-7b2456aed6e7",
   "a6096fb3-2a06-474a-88e6-98196c6bed7b",
   "119a0d88-2237-49f0-b098-977f1f4ebdc5",
   "41be14a9-ed0c-4d32-b10e-793288ce019d",
   "88130bb9-1535-4c6a-9b22-bda0cf09164d",
   "9f8e079d-264b-43af-93c8-5281a0f9c8e8",
   "14987944-d6cd-492e-87e9-a35a0cddef01",
   "2bb4e30e-aa0d-4384-9c3f-76fe5e567975",
   "5e3c73b3-0cb5-4b8e-8b4a-829edb8faa17",
   "a7282b63-2d99-4030-b7d2-281c58f36be3",
   "90b4fb37-5d22-4c87-bd09-3d778956685f",
   "4bd196f3-7025-4847-8421-abc294ad4837",
   "30fa362b-8c75-4c03-ac5f-0d39b5fabb66",
   "7f3a4d6e-c0e3-414a-b211-ab88d1372928",
   "2ff2f2db-305a-4fbe-a6fc-4fa2251baaf5",
   "ca2e4421-f0db-4ecd-82a2-d39cd1462f01",
   "05ad2453-dec3-47bd-9db7-36847427b3b0",
   "eef2cb30-7d42-4432-8d78-fe9253481774",
   "e2a3bf6e-94ee-43e1-a3e9-05db370cdce8",
   "4c376f5e-e370-4018-b01e-378c4992d80e",
   "37d245fb-3dd7-45f0-aaef-16795820fa21",
   "fe8b96a5-8d70-470f-a45f-b410b8dceed6",
   "6fe9279a-e3d6-4d90-80d9-7ab397e822c3",
   "b1248cee-81d3-4f96-b07b-50b9c1575b7b",
   "a183896b-52a6-452d-b6c1-48d9ddc0a3bf",
   "5b80e836-a915-430d-bffa-138c846790b6",
   "7c73a02a-c487-4c54-9b16-1ec513578403",
   "fb5f8321-81e4-4e87-be82-6940a728dbc5",
   "1b0061b0-42e3-4882-bd51-4c3c3940737d",
   "d974d0fb-6a25-4f0f-bcf2-c27ccb01b664"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
